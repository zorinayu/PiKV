"""
PiKV Advanced Methods Usage Example
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ–°çš„EPLB routingå’Œadvanced distillationæ–¹æ³•

USAGE Example:
cd /Users/dongliu/Documents/GitHub/PiKV && python -c "import sys; sys.path.append('.'); from core.single.pikv_routing import EPLBRouter; from core.single.advanced_distillation import AdvancedDistillationManager, DistillationMethod; import torch; print('Testing EPLB Router...'); router = EPLBRouter(hidden_size=512, num_experts=8, top_k=2); hidden_states = torch.randn(2, 64, 512); dispatch, combine, probs, loss = router(hidden_states); print(f'EPLB Router test passed! Loss: {loss.item():.4f}'); print('Testing Advanced Distillation...'); distill_manager = AdvancedDistillationManager(teacher_hidden_size=768, student_hidden_size=512, method=DistillationMethod.DISTILLM, num_layers=3); print('Advanced Distillation test passed!'); print('All new methods are working correctly!')"
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
from typing import List, Dict, Optional

# å¯¼å…¥PiKVç»„ä»¶
from core.single.pikv_moe import PiKVMoE
from core.single.pikv_routing import EPLBRouter, HierarchicalRouter
from core.single.advanced_distillation import AdvancedDistillationManager, DistillationMethod
from core.single.cache_scheduling import SchedulingPolicy


class AdvancedPiKVExample:
    """
    é«˜çº§PiKVæ–¹æ³•ä½¿ç”¨ç¤ºä¾‹
    """
    
    def __init__(
        self,
        vocab_size: int = 1000,
        hidden_size: int = 512,
        num_experts: int = 8,
        num_layers: int = 6,
        use_eplb_routing: bool = True,
        use_advanced_distillation: bool = True,
        distillation_method: DistillationMethod = DistillationMethod.DISTILLM_2
    ):
        self.vocab_size = vocab_size
        self.hidden_size = hidden_size
        self.num_experts = num_experts
        self.num_layers = num_layers
        self.use_eplb_routing = use_eplb_routing
        self.use_advanced_distillation = use_advanced_distillation
        self.distillation_method = distillation_method
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._setup_models()
        
        # åˆå§‹åŒ–è’¸é¦å™¨ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        if self.use_advanced_distillation:
            self._setup_distillation()
    
    def _setup_models(self):
        """è®¾ç½®æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹"""
        print("åˆå§‹åŒ–æ¨¡å‹...")
        
        # æ•™å¸ˆæ¨¡å‹ï¼ˆæ›´å¤§ï¼‰
        teacher_config = {
            'vocab_size': self.vocab_size,
            'hidden_size': self.hidden_size * 2,  # æ•™å¸ˆæ¨¡å‹æ›´å¤§
            'num_experts': self.num_experts,
            'num_layers': self.num_layers,
            'top_k': 2,
            'use_cache_scheduling': True,
            'cache_scheduling_policy': SchedulingPolicy.H2O
        }
        
        # å­¦ç”Ÿæ¨¡å‹ï¼ˆè¾ƒå°ï¼‰
        student_config = {
            'vocab_size': self.vocab_size,
            'hidden_size': self.hidden_size,
            'num_experts': self.num_experts // 2,  # å­¦ç”Ÿæ¨¡å‹ä¸“å®¶æ›´å°‘
            'num_layers': self.num_layers // 2,    # å­¦ç”Ÿæ¨¡å‹å±‚æ•°æ›´å°‘
            'top_k': 2,
            'use_cache_scheduling': True,
            'cache_scheduling_policy': SchedulingPolicy.STREAMING_LLM
        }
        
        # å¦‚æœä½¿ç”¨EPLB routingï¼Œåˆ›å»ºè‡ªå®šä¹‰è·¯ç”±å™¨
        if self.use_eplb_routing:
            print("ä½¿ç”¨EPLBè·¯ç”±å™¨...")
            teacher_router = EPLBRouter(
                hidden_size=teacher_config['hidden_size'],
                num_experts=teacher_config['num_experts'],
                top_k=teacher_config['top_k'],
                temperature=1.0,
                balance_coefficient=0.01,
                use_auxiliary_loss=True,
                use_z_loss=True
            )
            
            student_router = EPLBRouter(
                hidden_size=student_config['hidden_size'],
                num_experts=student_config['num_experts'],
                top_k=student_config['top_k'],
                temperature=1.0,
                balance_coefficient=0.01,
                use_auxiliary_loss=True,
                use_z_loss=True
            )
            
            # å°†è‡ªå®šä¹‰è·¯ç”±å™¨ä¼ é€’ç»™æ¨¡å‹ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
            teacher_config['custom_router'] = teacher_router
            student_config['custom_router'] = student_router
        
        # åˆ›å»ºæ¨¡å‹
        self.teacher_model = PiKVMoE(**teacher_config).to(self.device)
        self.student_model = PiKVMoE(**student_config).to(self.device)
        
        print(f"æ•™å¸ˆæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.teacher_model.parameters()):,}")
        print(f"å­¦ç”Ÿæ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in self.student_model.parameters()):,}")
    
    def _setup_distillation(self):
        """è®¾ç½®é«˜çº§è’¸é¦"""
        print(f"åˆå§‹åŒ–é«˜çº§è’¸é¦: {self.distillation_method.value}")
        
        self.distillation_manager = AdvancedDistillationManager(
            teacher_hidden_size=self.hidden_size * 2,
            student_hidden_size=self.hidden_size,
            method=self.distillation_method,
            num_layers=self.num_layers // 2,
            temperature=4.0,
            alpha=0.7,
            beta=0.3
        ).to(self.device)
        
        # è·å–æ–¹æ³•ä¿¡æ¯
        method_info = self.distillation_manager.get_method_info()
        print(f"è’¸é¦æ–¹æ³•: {method_info['name']}")
        print(f"æè¿°: {method_info['description']}")
    
    def create_sample_data(self, batch_size: int = 8, seq_len: int = 128, num_batches: int = 100):
        """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
        print(f"åˆ›å»ºç¤ºä¾‹æ•°æ®: {num_batches} batches, batch_size={batch_size}, seq_len={seq_len}")
        
        # ç”Ÿæˆéšæœºåºåˆ—æ•°æ®
        input_ids = torch.randint(0, self.vocab_size, (num_batches * batch_size, seq_len))
        labels = torch.randint(0, self.vocab_size, (num_batches * batch_size, seq_len))
        
        # åˆ›å»ºæ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
        dataset = TensorDataset(input_ids, labels)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
        
        return dataloader
    
    def train_with_advanced_methods(
        self, 
        dataloader: DataLoader, 
        num_epochs: int = 5,
        learning_rate: float = 1e-4
    ):
        """ä½¿ç”¨é«˜çº§æ–¹æ³•è¿›è¡Œè®­ç»ƒ"""
        print(f"å¼€å§‹è®­ç»ƒ {num_epochs} epochs...")
        
        # è®¾ç½®ä¼˜åŒ–å™¨
        teacher_optimizer = optim.AdamW(self.teacher_model.parameters(), lr=learning_rate)
        student_optimizer = optim.AdamW(self.student_model.parameters(), lr=learning_rate)
        
        # è®­ç»ƒå†å²
        history = {
            'teacher_loss': [],
            'student_loss': [],
            'distillation_loss': [],
            'routing_loss': [],
            'expert_usage_variance': []
        }
        
        for epoch in range(num_epochs):
            epoch_teacher_loss = 0.0
            epoch_student_loss = 0.0
            epoch_distill_loss = 0.0
            epoch_routing_loss = 0.0
            epoch_expert_variance = 0.0
            
            num_batches = 0
            
            for batch_idx, (input_ids, labels) in enumerate(dataloader):
                input_ids = input_ids.to(self.device)
                labels = labels.to(self.device)
                
                # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
                teacher_optimizer.zero_grad()
                teacher_outputs = self.teacher_model(input_ids)
                teacher_logits = teacher_outputs['logits'] if isinstance(teacher_outputs, dict) else teacher_outputs
                
                # æ•™å¸ˆæ¨¡å‹æŸå¤±
                teacher_loss = nn.CrossEntropyLoss()(
                    teacher_logits.view(-1, self.vocab_size), 
                    labels.view(-1)
                )
                
                # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
                student_optimizer.zero_grad()
                student_outputs = self.student_model(input_ids)
                student_logits = student_outputs['logits'] if isinstance(student_outputs, dict) else student_outputs
                
                # å­¦ç”Ÿæ¨¡å‹æŸå¤±
                student_loss = nn.CrossEntropyLoss()(
                    student_logits.view(-1, self.vocab_size), 
                    labels.view(-1)
                )
                
                total_loss = teacher_loss + student_loss
                
                # é«˜çº§è’¸é¦ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if self.use_advanced_distillation:
                    # æå–ç‰¹å¾ï¼ˆç®€åŒ–å¤„ç†ï¼‰
                    teacher_features = [torch.randn_like(input_ids, dtype=torch.float).unsqueeze(-1).expand(-1, -1, self.hidden_size * 2) for _ in range(self.num_layers // 2)]
                    student_features = [torch.randn_like(input_ids, dtype=torch.float).unsqueeze(-1).expand(-1, -1, self.hidden_size) for _ in range(self.num_layers // 2)]
                    
                    # æ‰§è¡Œè’¸é¦
                    distill_result = self.distillation_manager.distill(
                        student_features=student_features,
                        teacher_features=teacher_features,
                        student_logits=student_logits,
                        teacher_logits=teacher_logits,
                        labels=labels
                    )
                    
                    distill_loss = distill_result['total_loss']
                    total_loss += 0.5 * distill_loss
                    epoch_distill_loss += distill_loss.item()
                
                # è·¯ç”±æŸå¤±ï¼ˆå¦‚æœä½¿ç”¨EPLBï¼‰
                routing_loss = 0.0
                if self.use_eplb_routing and hasattr(self.teacher_model, 'get_routing_loss'):
                    routing_loss = self.teacher_model.get_routing_loss()
                    total_loss += 0.1 * routing_loss
                    epoch_routing_loss += routing_loss.item()
                
                # åå‘ä¼ æ’­
                total_loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.teacher_model.parameters(), 1.0)
                torch.nn.utils.clip_grad_norm_(self.student_model.parameters(), 1.0)
                
                # ä¼˜åŒ–å™¨æ­¥éª¤
                teacher_optimizer.step()
                student_optimizer.step()
                
                # è®°å½•æŸå¤±
                epoch_teacher_loss += teacher_loss.item()
                epoch_student_loss += student_loss.item()
                
                # è®¡ç®—ä¸“å®¶ä½¿ç”¨æ–¹å·®ï¼ˆç®€åŒ–ï¼‰
                if hasattr(self.teacher_model, 'get_expert_usage'):
                    expert_usage = self.teacher_model.get_expert_usage()
                    if expert_usage is not None:
                        epoch_expert_variance += expert_usage.var().item()
                
                num_batches += 1
                
                # æ‰“å°è¿›åº¦
                if batch_idx % 10 == 0:
                    print(f"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, "
                          f"Teacher Loss: {teacher_loss.item():.4f}, "
                          f"Student Loss: {student_loss.item():.4f}")
            
            # è®°å½•epochå¹³å‡æŸå¤±
            history['teacher_loss'].append(epoch_teacher_loss / num_batches)
            history['student_loss'].append(epoch_student_loss / num_batches)
            history['distillation_loss'].append(epoch_distill_loss / num_batches)
            history['routing_loss'].append(epoch_routing_loss / num_batches)
            history['expert_usage_variance'].append(epoch_expert_variance / num_batches)
            
            print(f"Epoch {epoch+1} å®Œæˆ:")
            print(f"  å¹³å‡æ•™å¸ˆæŸå¤±: {history['teacher_loss'][-1]:.4f}")
            print(f"  å¹³å‡å­¦ç”ŸæŸå¤±: {history['student_loss'][-1]:.4f}")
            if self.use_advanced_distillation:
                print(f"  å¹³å‡è’¸é¦æŸå¤±: {history['distillation_loss'][-1]:.4f}")
            if self.use_eplb_routing:
                print(f"  å¹³å‡è·¯ç”±æŸå¤±: {history['routing_loss'][-1]:.4f}")
                print(f"  ä¸“å®¶ä½¿ç”¨æ–¹å·®: {history['expert_usage_variance'][-1]:.4f}")
        
        return history
    
    def evaluate_models(self, dataloader: DataLoader):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("è¯„ä¼°æ¨¡å‹æ€§èƒ½...")
        
        self.teacher_model.eval()
        self.student_model.eval()
        
        teacher_total_loss = 0.0
        student_total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for input_ids, labels in dataloader:
                input_ids = input_ids.to(self.device)
                labels = labels.to(self.device)
                
                # æ•™å¸ˆæ¨¡å‹
                teacher_outputs = self.teacher_model(input_ids)
                teacher_logits = teacher_outputs['logits'] if isinstance(teacher_outputs, dict) else teacher_outputs
                teacher_loss = nn.CrossEntropyLoss()(
                    teacher_logits.view(-1, self.vocab_size), 
                    labels.view(-1)
                )
                
                # å­¦ç”Ÿæ¨¡å‹
                student_outputs = self.student_model(input_ids)
                student_logits = student_outputs['logits'] if isinstance(student_outputs, dict) else student_outputs
                student_loss = nn.CrossEntropyLoss()(
                    student_logits.view(-1, self.vocab_size), 
                    labels.view(-1)
                )
                
                teacher_total_loss += teacher_loss.item()
                student_total_loss += student_loss.item()
                num_batches += 1
        
        avg_teacher_loss = teacher_total_loss / num_batches
        avg_student_loss = student_total_loss / num_batches
        
        print(f"è¯„ä¼°ç»“æœ:")
        print(f"  æ•™å¸ˆæ¨¡å‹å¹³å‡æŸå¤±: {avg_teacher_loss:.4f}")
        print(f"  å­¦ç”Ÿæ¨¡å‹å¹³å‡æŸå¤±: {avg_student_loss:.4f}")
        print(f"  æ€§èƒ½æ¯”ç‡ (å­¦ç”Ÿ/æ•™å¸ˆ): {avg_student_loss/avg_teacher_loss:.2f}")
        
        return avg_teacher_loss, avg_student_loss
    
    def demonstrate_cache_scheduling(self):
        """æ¼”ç¤ºç¼“å­˜è°ƒåº¦åŠŸèƒ½"""
        print("\næ¼”ç¤ºç¼“å­˜è°ƒåº¦åŠŸèƒ½...")
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        test_input = torch.randint(0, self.vocab_size, (2, 64)).to(self.device)
        
        # æµ‹è¯•ä¸åŒçš„è°ƒåº¦ç­–ç•¥
        scheduling_policies = [
            SchedulingPolicy.NONE,
            SchedulingPolicy.H2O,
            SchedulingPolicy.STREAMING_LLM,
            SchedulingPolicy.QUEST,
            SchedulingPolicy.LRU
        ]
        
        for policy in scheduling_policies:
            print(f"\næµ‹è¯•è°ƒåº¦ç­–ç•¥: {policy.value}")
            
            # æ›´æ”¹è°ƒåº¦ç­–ç•¥
            self.student_model.change_cache_scheduling_policy(policy)
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.student_model(test_input)
            
            # è·å–ç¼“å­˜ç»Ÿè®¡
            cache_stats = self.student_model.get_cache_stats()
            if cache_stats:
                print(f"  ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 0):.2%}")
                print(f"  ç¼“å­˜ä½¿ç”¨ç‡: {cache_stats.get('utilization', 0):.2%}")
    
    def save_models(self, save_dir: str = "checkpoints"):
        """ä¿å­˜æ¨¡å‹"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        teacher_path = os.path.join(save_dir, "teacher_model.pt")
        student_path = os.path.join(save_dir, "student_model.pt")
        
        torch.save(self.teacher_model.state_dict(), teacher_path)
        torch.save(self.student_model.state_dict(), student_path)
        
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ° {save_dir}")


def main():
    """ä¸»å‡½æ•°"""
    print("=== PiKV Advanced Methods Example ===\n")
    
    # åˆ›å»ºç¤ºä¾‹å®ä¾‹
    example = AdvancedPiKVExample(
        vocab_size=1000,
        hidden_size=256,  # è¾ƒå°çš„å°ºå¯¸ç”¨äºæ¼”ç¤º
        num_experts=8,
        num_layers=4,
        use_eplb_routing=True,
        use_advanced_distillation=True,
        distillation_method=DistillationMethod.DISTILLM_2
    )
    
    # åˆ›å»ºæ•°æ®
    train_dataloader = example.create_sample_data(
        batch_size=4, 
        seq_len=64, 
        num_batches=20
    )
    
    test_dataloader = example.create_sample_data(
        batch_size=4, 
        seq_len=64, 
        num_batches=5
    )
    
    # è®­ç»ƒæ¨¡å‹
    print("\n" + "="*50)
    history = example.train_with_advanced_methods(
        train_dataloader, 
        num_epochs=3,
        learning_rate=1e-4
    )
    
    # è¯„ä¼°æ¨¡å‹
    print("\n" + "="*50)
    example.evaluate_models(test_dataloader)
    
    # æ¼”ç¤ºç¼“å­˜è°ƒåº¦
    print("\n" + "="*50)
    example.demonstrate_cache_scheduling()
    
    # ä¿å­˜æ¨¡å‹
    print("\n" + "="*50)
    example.save_models()
    
    print("\nğŸ‰ ç¤ºä¾‹å®Œæˆï¼")
    
    # æ‰“å°æ€»ç»“
    print("\n=== æ€»ç»“ ===")
    print("æœ¬ç¤ºä¾‹å±•ç¤ºäº†ä»¥ä¸‹é«˜çº§åŠŸèƒ½ï¼š")
    print("1. EPLB (Expert-level Load Balancing) è·¯ç”±ç­–ç•¥")
    print("2. DistillM-2 é«˜çº§çŸ¥è¯†è’¸é¦")
    print("3. å¤šç§ç¼“å­˜è°ƒåº¦ç­–ç•¥ (H2O, StreamingLLM, QUEST, LRU)")
    print("4. æ•™å¸ˆ-å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ")
    print("5. æ¨¡å‹æ€§èƒ½è¯„ä¼°å’Œä¿å­˜")


if __name__ == "__main__":
    main() 