"""
æµ‹è¯•æ–°çš„é«˜çº§æ–¹æ³•ï¼šEPLB Routing å’Œ Advanced Distillation
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Dict
import matplotlib.pyplot as plt
import numpy as np

# å¯¼å…¥æ–°çš„æ–¹æ³•
from pikv_routing import EPLBRouter, HierarchicalRouter
from advanced_distillation import (
    AdvancedDistillationManager, 
    DistillationMethod,
    DistillMDistillation,
    DistillM2Distillation,
    SpeculativeKDDistillation
)


def test_eplb_router():
    """æµ‹è¯•EPLBè·¯ç”±å™¨"""
    print("=== æµ‹è¯• EPLB Router ===")
    
    # å‚æ•°è®¾ç½®
    batch_size = 4
    seq_len = 128
    hidden_size = 512
    num_experts = 8
    top_k = 2
    
    # åˆ›å»ºEPLBè·¯ç”±å™¨
    router = EPLBRouter(
        hidden_size=hidden_size,
        num_experts=num_experts,
        top_k=top_k,
        temperature=1.0,
        balance_coefficient=0.01,
        use_auxiliary_loss=True,
        use_z_loss=True
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    hidden_states = torch.randn(batch_size, seq_len, hidden_size)
    
    # å‰å‘ä¼ æ’­
    dispatch_tensor, combine_tensor, router_probs, aux_loss = router(hidden_states)
    
    print(f"è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
    print(f"è°ƒåº¦å¼ é‡å½¢çŠ¶: {dispatch_tensor.shape}")
    print(f"ç»„åˆå¼ é‡å½¢çŠ¶: {combine_tensor.shape}")
    print(f"è·¯ç”±æ¦‚ç‡å½¢çŠ¶: {router_probs.shape}")
    print(f"è¾…åŠ©æŸå¤±: {aux_loss.item():.4f}")
    
    # åˆ†æä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ
    expert_usage = router_probs.mean(dim=[0, 1])
    print(f"ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ: {expert_usage}")
    print(f"ä¸“å®¶ä½¿ç”¨æ–¹å·®: {expert_usage.var().item():.4f}")
    
    # æµ‹è¯•å¤šæ¬¡å‰å‘ä¼ æ’­ï¼Œè§‚å¯Ÿè´Ÿè½½å¹³è¡¡æ•ˆæœ
    usage_history = []
    for i in range(10):
        hidden_states = torch.randn(batch_size, seq_len, hidden_size)
        _, _, router_probs, _ = router(hidden_states)
        expert_usage = router_probs.mean(dim=[0, 1])
        usage_history.append(expert_usage.detach().numpy())
    
    # å¯è§†åŒ–ä¸“å®¶ä½¿ç”¨è¶‹åŠ¿
    usage_history = np.array(usage_history)
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    for i in range(num_experts):
        plt.plot(usage_history[:, i], label=f'Expert {i}')
    plt.title('Expert Usage Over Time (EPLB)')
    plt.xlabel('Iteration')
    plt.ylabel('Usage Probability')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    final_usage = usage_history[-1]
    plt.bar(range(num_experts), final_usage)
    plt.title('Final Expert Usage Distribution')
    plt.xlabel('Expert ID')
    plt.ylabel('Usage Probability')
    plt.axhline(y=1/num_experts, color='r', linestyle='--', label='Uniform')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('eplb_router_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("EPLB Router æµ‹è¯•å®Œæˆï¼")


def test_hierarchical_router():
    """æµ‹è¯•åˆ†å±‚è·¯ç”±å™¨"""
    print("\n=== æµ‹è¯• Hierarchical Router ===")
    
    # å‚æ•°è®¾ç½®
    batch_size = 4
    seq_len = 128
    hidden_size = 512
    num_experts = 16  # æ›´å¤šä¸“å®¶ä»¥å±•ç¤ºåˆ†å±‚æ•ˆæœ
    num_groups = 4
    top_k = 2
    
    # åˆ›å»ºåˆ†å±‚è·¯ç”±å™¨
    router = HierarchicalRouter(
        hidden_size=hidden_size,
        num_experts=num_experts,
        top_k=top_k,
        num_groups=num_groups,
        group_top_k=1
    )
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    hidden_states = torch.randn(batch_size, seq_len, hidden_size)
    
    # å‰å‘ä¼ æ’­
    dispatch_tensor, combine_tensor, router_probs, aux_loss = router(hidden_states)
    
    print(f"è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
    print(f"ä¸“å®¶æ€»æ•°: {num_experts}, ç»„æ•°: {num_groups}, æ¯ç»„ä¸“å®¶æ•°: {num_experts // num_groups}")
    print(f"è°ƒåº¦å¼ é‡å½¢çŠ¶: {dispatch_tensor.shape}")
    print(f"ç»„åˆå¼ é‡å½¢çŠ¶: {combine_tensor.shape}")
    print(f"è·¯ç”±æ¦‚ç‡å½¢çŠ¶: {router_probs.shape}")
    print(f"è¾…åŠ©æŸå¤±: {aux_loss.item():.4f}")
    
    # åˆ†æç»„çº§å’Œä¸“å®¶çº§ä½¿ç”¨åˆ†å¸ƒ
    expert_usage = router_probs.mean(dim=[0, 1])
    group_usage = expert_usage.view(num_groups, -1).sum(dim=1)
    
    print(f"ç»„ä½¿ç”¨åˆ†å¸ƒ: {group_usage}")
    print(f"ä¸“å®¶ä½¿ç”¨åˆ†å¸ƒ: {expert_usage}")
    
    # å¯è§†åŒ–åˆ†å±‚ä½¿ç”¨åˆ†å¸ƒ
    plt.figure(figsize=(12, 6))
    
    plt.subplot(1, 2, 1)
    plt.bar(range(num_groups), group_usage.detach().numpy())
    plt.title('Group Usage Distribution')
    plt.xlabel('Group ID')
    plt.ylabel('Usage Probability')
    plt.axhline(y=1/num_groups, color='r', linestyle='--', label='Uniform')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 2, 2)
    expert_usage_np = expert_usage.detach().numpy()
    colors = ['C0', 'C1', 'C2', 'C3'] * (num_experts // num_groups)
    plt.bar(range(num_experts), expert_usage_np, color=colors)
    plt.title('Expert Usage Distribution (Colored by Group)')
    plt.xlabel('Expert ID')
    plt.ylabel('Usage Probability')
    plt.axhline(y=1/num_experts, color='r', linestyle='--', label='Uniform')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('hierarchical_router_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("Hierarchical Router æµ‹è¯•å®Œæˆï¼")


def test_advanced_distillation():
    """æµ‹è¯•é«˜çº§è’¸é¦æ–¹æ³•"""
    print("\n=== æµ‹è¯• Advanced Distillation Methods ===")
    
    # å‚æ•°è®¾ç½®
    batch_size = 4
    seq_len = 128
    teacher_hidden_size = 768
    student_hidden_size = 512
    vocab_size = 1000
    num_layers = 3
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„æ•™å¸ˆå’Œå­¦ç”Ÿç‰¹å¾
    teacher_features = [
        torch.randn(batch_size, seq_len, teacher_hidden_size) 
        for _ in range(num_layers)
    ]
    student_features = [
        torch.randn(batch_size, seq_len, student_hidden_size) 
        for _ in range(num_layers)
    ]
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„logits
    teacher_logits = torch.randn(batch_size, seq_len, vocab_size)
    student_logits = torch.randn(batch_size, seq_len, vocab_size)
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    # æµ‹è¯•ä¸åŒçš„è’¸é¦æ–¹æ³•
    methods = [
        DistillationMethod.DISTILLM,
        DistillationMethod.DISTILLM_2,
        DistillationMethod.SPECULATIVE_KD
    ]
    
    results = {}
    
    for method in methods:
        print(f"\n--- æµ‹è¯• {method.value} ---")
        
        # åˆ›å»ºè’¸é¦ç®¡ç†å™¨
        distill_manager = AdvancedDistillationManager(
            teacher_hidden_size=teacher_hidden_size,
            student_hidden_size=student_hidden_size,
            method=method,
            num_layers=num_layers
        )
        
        # æ‰§è¡Œè’¸é¦
        loss_dict = distill_manager.distill(
            student_features=student_features,
            teacher_features=teacher_features,
            student_logits=student_logits,
            teacher_logits=teacher_logits,
            labels=labels
        )
        
        # è®°å½•ç»“æœ
        results[method.value] = loss_dict
        
        # æ‰“å°æŸå¤±ä¿¡æ¯
        print(f"æ€»æŸå¤±: {loss_dict['total_loss'].item():.4f}")
        print(f"è’¸é¦æŸå¤±: {loss_dict['distill_loss'].item():.4f}")
        print(f"KLæŸå¤±: {loss_dict['kl_loss'].item():.4f}")
        
        # æ‰“å°æ–¹æ³•ç‰¹æœ‰çš„æŸå¤±
        if 'feature_loss' in loss_dict:
            print(f"ç‰¹å¾æŸå¤±: {loss_dict['feature_loss'].item():.4f}")
        if 'discriminative_loss' in loss_dict:
            print(f"åˆ¤åˆ«æ€§æŸå¤±: {loss_dict['discriminative_loss'].item():.4f}")
        if 'multi_scale_loss' in loss_dict:
            print(f"å¤šå°ºåº¦æŸå¤±: {loss_dict['multi_scale_loss'].item():.4f}")
        if 'attention_loss' in loss_dict:
            print(f"æ³¨æ„åŠ›æŸå¤±: {loss_dict['attention_loss'].item():.4f}")
        if 'speculation_loss' in loss_dict:
            print(f"æŠ•æœºæŸå¤±: {loss_dict['speculation_loss'].item():.4f}")
        if 'verification_loss' in loss_dict:
            print(f"éªŒè¯æŸå¤±: {loss_dict['verification_loss'].item():.4f}")
        if 'prediction_accuracy' in loss_dict:
            print(f"é¢„æµ‹å‡†ç¡®ç‡: {loss_dict['prediction_accuracy'].item():.4f}")
        
        # è·å–æ–¹æ³•ä¿¡æ¯
        method_info = distill_manager.get_method_info()
        print(f"æ–¹æ³•æè¿°: {method_info['description']}")
    
    # å¯è§†åŒ–ä¸åŒæ–¹æ³•çš„æŸå¤±æ¯”è¾ƒ
    plt.figure(figsize=(15, 10))
    
    # æ€»æŸå¤±æ¯”è¾ƒ
    plt.subplot(2, 3, 1)
    methods_names = list(results.keys())
    total_losses = [results[method]['total_loss'].item() for method in methods_names]
    plt.bar(methods_names, total_losses)
    plt.title('Total Loss Comparison')
    plt.ylabel('Loss')
    plt.xticks(rotation=45)
    plt.grid(True)
    
    # è’¸é¦æŸå¤±æ¯”è¾ƒ
    plt.subplot(2, 3, 2)
    distill_losses = [results[method]['distill_loss'].item() for method in methods_names]
    plt.bar(methods_names, distill_losses)
    plt.title('Distillation Loss Comparison')
    plt.ylabel('Loss')
    plt.xticks(rotation=45)
    plt.grid(True)
    
    # KLæŸå¤±æ¯”è¾ƒ
    plt.subplot(2, 3, 3)
    kl_losses = [results[method]['kl_loss'].item() for method in methods_names]
    plt.bar(methods_names, kl_losses)
    plt.title('KL Loss Comparison')
    plt.ylabel('Loss')
    plt.xticks(rotation=45)
    plt.grid(True)
    
    # ç‰¹å¾ç›¸å…³æŸå¤±
    plt.subplot(2, 3, 4)
    feature_losses = []
    feature_methods = []
    for method in methods_names:
        if 'feature_loss' in results[method]:
            feature_losses.append(results[method]['feature_loss'].item())
            feature_methods.append(method)
        elif 'multi_scale_loss' in results[method]:
            feature_losses.append(results[method]['multi_scale_loss'].item())
            feature_methods.append(method)
    
    if feature_losses:
        plt.bar(feature_methods, feature_losses)
        plt.title('Feature-related Loss')
        plt.ylabel('Loss')
        plt.xticks(rotation=45)
        plt.grid(True)
    
    # æ–¹æ³•ç‰¹æœ‰æŸå¤±
    plt.subplot(2, 3, 5)
    special_losses = []
    special_methods = []
    special_labels = []
    
    for method in methods_names:
        if 'discriminative_loss' in results[method]:
            special_losses.append(results[method]['discriminative_loss'].item())
            special_methods.append(method)
            special_labels.append('Discriminative')
        elif 'speculation_loss' in results[method]:
            special_losses.append(results[method]['speculation_loss'].item())
            special_methods.append(method)
            special_labels.append('Speculation')
        elif 'attention_loss' in results[method]:
            special_losses.append(results[method]['attention_loss'].item())
            special_methods.append(method)
            special_labels.append('Attention')
    
    if special_losses:
        bars = plt.bar(special_methods, special_losses)
        plt.title('Method-specific Losses')
        plt.ylabel('Loss')
        plt.xticks(rotation=45)
        plt.grid(True)
        
        # æ·»åŠ æ ‡ç­¾
        for bar, label in zip(bars, special_labels):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,
                    label, ha='center', va='bottom', fontsize=8)
    
    # é¢„æµ‹å‡†ç¡®ç‡ï¼ˆå¦‚æœæœ‰ï¼‰
    plt.subplot(2, 3, 6)
    accuracies = []
    acc_methods = []
    for method in methods_names:
        if 'prediction_accuracy' in results[method]:
            accuracies.append(results[method]['prediction_accuracy'].item())
            acc_methods.append(method)
    
    if accuracies:
        plt.bar(acc_methods, accuracies)
        plt.title('Prediction Accuracy')
        plt.ylabel('Accuracy')
        plt.xticks(rotation=45)
        plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('advanced_distillation_comparison.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("\nAdvanced Distillation æµ‹è¯•å®Œæˆï¼")


def test_integration():
    """æµ‹è¯•EPLBè·¯ç”±å™¨ä¸é«˜çº§è’¸é¦çš„é›†æˆ"""
    print("\n=== æµ‹è¯• EPLB + Advanced Distillation é›†æˆ ===")
    
    # å‚æ•°è®¾ç½®
    batch_size = 4
    seq_len = 64
    hidden_size = 512
    num_experts = 8
    top_k = 2
    vocab_size = 1000
    
    # åˆ›å»ºEPLBè·¯ç”±å™¨
    router = EPLBRouter(
        hidden_size=hidden_size,
        num_experts=num_experts,
        top_k=top_k,
        temperature=1.0,
        balance_coefficient=0.01
    )
    
    # åˆ›å»ºé«˜çº§è’¸é¦ç®¡ç†å™¨
    distill_manager = AdvancedDistillationManager(
        teacher_hidden_size=hidden_size,
        student_hidden_size=hidden_size,
        method=DistillationMethod.DISTILLM_2,
        num_layers=2
    )
    
    # æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯
    print("æ¨¡æ‹Ÿè®­ç»ƒå¾ªç¯...")
    routing_losses = []
    distill_losses = []
    expert_usage_variance = []
    
    for epoch in range(10):
        # åˆ›å»ºæ‰¹æ¬¡æ•°æ®
        hidden_states = torch.randn(batch_size, seq_len, hidden_size)
        teacher_features = [torch.randn(batch_size, seq_len, hidden_size) for _ in range(2)]
        student_features = [torch.randn(batch_size, seq_len, hidden_size) for _ in range(2)]
        teacher_logits = torch.randn(batch_size, seq_len, vocab_size)
        student_logits = torch.randn(batch_size, seq_len, vocab_size)
        
        # è·¯ç”±è®¡ç®—
        _, _, router_probs, routing_loss = router(hidden_states)
        
        # è’¸é¦è®¡ç®—
        distill_result = distill_manager.distill(
            student_features=student_features,
            teacher_features=teacher_features,
            student_logits=student_logits,
            teacher_logits=teacher_logits
        )
        
        # è®°å½•æŒ‡æ ‡
        routing_losses.append(routing_loss.item())
        distill_losses.append(distill_result['total_loss'].item())
        
        expert_usage = router_probs.mean(dim=[0, 1])
        expert_usage_variance.append(expert_usage.var().item())
        
        if epoch % 2 == 0:
            print(f"Epoch {epoch}: Routing Loss = {routing_loss.item():.4f}, "
                  f"Distill Loss = {distill_result['total_loss'].item():.4f}, "
                  f"Expert Variance = {expert_usage.var().item():.4f}")
    
    # å¯è§†åŒ–é›†æˆè®­ç»ƒè¿‡ç¨‹
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(routing_losses, 'b-', label='Routing Loss')
    plt.title('Routing Loss Over Training')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 3, 2)
    plt.plot(distill_losses, 'r-', label='Distillation Loss')
    plt.title('Distillation Loss Over Training')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    plt.subplot(1, 3, 3)
    plt.plot(expert_usage_variance, 'g-', label='Expert Usage Variance')
    plt.title('Expert Load Balancing Over Training')
    plt.xlabel('Epoch')
    plt.ylabel('Variance')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('integration_training.png', dpi=150, bbox_inches='tight')
    plt.show()
    
    print("é›†æˆæµ‹è¯•å®Œæˆï¼")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•æ–°çš„é«˜çº§æ–¹æ³•...")
    
    # è®¾ç½®éšæœºç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§
    torch.manual_seed(42)
    np.random.seed(42)
    
    try:
        # æµ‹è¯•EPLBè·¯ç”±å™¨
        test_eplb_router()
        
        # æµ‹è¯•åˆ†å±‚è·¯ç”±å™¨
        test_hierarchical_router()
        
        # æµ‹è¯•é«˜çº§è’¸é¦æ–¹æ³•
        test_advanced_distillation()
        
        # æµ‹è¯•é›†æˆ
        test_integration()
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("\nç”Ÿæˆçš„å›¾ç‰‡æ–‡ä»¶ï¼š")
        print("- eplb_router_analysis.png")
        print("- hierarchical_router_analysis.png") 
        print("- advanced_distillation_comparison.png")
        print("- integration_training.png")
        
    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 