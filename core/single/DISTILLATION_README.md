# PiKVçŸ¥è¯†è’¸é¦åŠŸèƒ½æ–‡æ¡£

## æ¦‚è¿°

PiKV MoEé›†æˆäº†å…ˆè¿›çš„çŸ¥è¯†è’¸é¦åŠŸèƒ½ï¼Œå…è®¸ä»å¤§å‹æ•™å¸ˆæ¨¡å‹å‘å°å‹å­¦ç”Ÿæ¨¡å‹è½¬ç§»çŸ¥è¯†ã€‚è¿™ä¸ªå®ç°åŸºäºPyTorchçš„çŸ¥è¯†è’¸é¦æ•™ç¨‹ï¼Œå¹¶é’ˆå¯¹MoEæ¶æ„è¿›è¡Œäº†ä¼˜åŒ–ã€‚

## ä¸»è¦ç‰¹æ€§

### ğŸ¯ å¤šå±‚æ¬¡è’¸é¦
- **æ ‡å‡†çŸ¥è¯†è’¸é¦**: è½¯ç›®æ ‡æŸå¤± (KLæ•£åº¦)
- **ç‰¹å¾åŒ¹é…**: ä¸­é—´å±‚ç‰¹å¾å¯¹é½
- **ä¸“å®¶çº§è’¸é¦**: MoEä¸“å®¶è¾“å‡ºå¯¹é½
- **KVç¼“å­˜è’¸é¦**: ç¼“å­˜çŠ¶æ€çŸ¥è¯†è½¬ç§»
- **è·¯ç”±è’¸é¦**: ä¸“å®¶é€‰æ‹©ç­–ç•¥å¯¹é½

### ğŸ”§ çµæ´»é…ç½®
- å¯è°ƒèŠ‚æ¸©åº¦å‚æ•°
- å¤šç§æŸå¤±æƒé‡é…ç½®
- åŠ¨æ€å¯ç”¨/ç¦ç”¨è’¸é¦
- æ”¯æŒæ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½

### âš¡ é«˜æ•ˆå®ç°
- æ•™å¸ˆæ¨¡å‹æ¢¯åº¦å†»ç»“
- æ¨ç†æ—¶å¯å…³é—­è’¸é¦
- ä¸LoRAæ— ç¼é›†æˆ

## å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ä½¿ç”¨

```python
import torch
from pikv_moe import PiKVMoE

# åˆ›å»ºå¸¦çŸ¥è¯†è’¸é¦çš„å­¦ç”Ÿæ¨¡å‹
student_model = PiKVMoE(
    rank=4,
    alpha=1.0,
    use_distillation=True,                    # å¯ç”¨çŸ¥è¯†è’¸é¦
    teacher_hidden_size=config['hidden_size'] * 2  # æ•™å¸ˆæ¨¡å‹æ›´å¤§
)

# ç”Ÿæˆè®­ç»ƒæ•°æ®
input_data = torch.randn(4, 32, config['hidden_size'])
targets = torch.randint(0, config['vocab_size'], (4, 32))

# è®¾ç½®ä¼˜åŒ–å™¨
optimizer = torch.optim.Adam(student_model.parameters(), lr=1e-4)

# è®­ç»ƒå¾ªç¯
for epoch in range(5):
    loss_info = student_model.distillation_step(
        input_data=input_data,
        targets=targets,
        optimizer=optimizer
    )
    
    print(f"Epoch {epoch+1}:")
    for loss_name, loss_value in loss_info.items():
        print(f"  {loss_name}: {loss_value:.4f}")
```

### æ¨ç†æ¨¡å¼

```python
# æ¨ç†æ—¶å…³é—­è’¸é¦ä»¥æé«˜é€Ÿåº¦
student_model.disable_distillation()
student_model.eval()

with torch.no_grad():
    output = student_model(input_data)
```

## APIæ–‡æ¡£

### PiKVMoEç±»

#### æ„é€ å‡½æ•°å‚æ•°

```python
PiKVMoE(
    rank=4,                          # LoRA rank
    alpha=1.0,                       # LoRA alpha
    use_distillation=False,          # æ˜¯å¦å¯ç”¨çŸ¥è¯†è’¸é¦
    teacher_hidden_size=None         # æ•™å¸ˆæ¨¡å‹éšè—å±‚å¤§å°
)
```

#### ä¸»è¦æ–¹æ³•

##### `distillation_step(input_data, targets=None, optimizer=None)`
æ‰§è¡Œä¸€æ­¥çŸ¥è¯†è’¸é¦è®­ç»ƒã€‚

**å‚æ•°:**
- `input_data`: è¾“å…¥å¼ é‡ `[batch_size, seq_len, hidden_size]`
- `targets`: ç›®æ ‡æ ‡ç­¾ `[batch_size, seq_len]` (å¯é€‰)
- `optimizer`: PyTorchä¼˜åŒ–å™¨ (å¯é€‰)

**è¿”å›:**
- `loss_info`: åŒ…å«å„é¡¹æŸå¤±çš„å­—å…¸

##### `enable_distillation(teacher_model_path=None)`
å¯ç”¨çŸ¥è¯†è’¸é¦åŠŸèƒ½ã€‚

**å‚æ•°:**
- `teacher_model_path`: æ•™å¸ˆæ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (å¯é€‰)

##### `disable_distillation()`
ç¦ç”¨çŸ¥è¯†è’¸é¦åŠŸèƒ½ï¼ˆæ¨ç†æ—¶ä½¿ç”¨ï¼‰ã€‚

##### `load_teacher_model(model_path)`
åŠ è½½é¢„è®­ç»ƒçš„æ•™å¸ˆæ¨¡å‹ã€‚

**å‚æ•°:**
- `model_path`: æ•™å¸ˆæ¨¡å‹æ–‡ä»¶è·¯å¾„

##### `save_checkpoint(path)` / `load_checkpoint(path)`
ä¿å­˜/åŠ è½½åŒ…å«è’¸é¦ç»„ä»¶çš„å®Œæ•´æ£€æŸ¥ç‚¹ã€‚

### è’¸é¦æ¨¡å—é…ç½®

#### æ¸©åº¦å‚æ•°è°ƒæ•´

```python
# ä¿®æ”¹è’¸é¦æ¸©åº¦ï¼ˆé»˜è®¤4.0ï¼‰
student_model.distillation_module.kd_loss.temperature = 6.0
```

#### æŸå¤±æƒé‡è°ƒæ•´

```python
# ä¿®æ”¹ä¸“å®¶è’¸é¦æƒé‡ï¼ˆé»˜è®¤0.4ï¼‰
student_model.distillation_module.expert_distill_weight = 0.5

# ä¿®æ”¹ç¼“å­˜è’¸é¦æƒé‡ï¼ˆé»˜è®¤0.3ï¼‰
student_model.distillation_module.cache_distill_weight = 0.4
```

## æŸå¤±å‡½æ•°è¯¦è§£

### 1. æ ‡å‡†çŸ¥è¯†è’¸é¦æŸå¤± (KD Loss)
```
KD_loss = KL_div(softmax(student_logits/T), softmax(teacher_logits/T)) * TÂ²
```
å…¶ä¸­Tæ˜¯æ¸©åº¦å‚æ•°ã€‚

### 2. ç¡¬ç›®æ ‡æŸå¤± (Hard Loss)
```
Hard_loss = CrossEntropy(student_logits, true_labels)
```

### 3. ç‰¹å¾åŒ¹é…æŸå¤± (Feature Loss)
```
Feature_loss = MSE(student_features, teacher_features)
```

### 4. ä¸“å®¶è’¸é¦æŸå¤± (Expert Loss)
```
Expert_loss = mean([MSE(student_expert_i, teacher_expert_i) for i in experts])
```

### 5. KVç¼“å­˜è’¸é¦æŸå¤± (Cache Loss)
```
Cache_loss = MSE(student_cache, teacher_cache) + attention_regularization
```

### 6. è·¯ç”±è’¸é¦æŸå¤± (Routing Loss)
```
Routing_loss = KL_div(student_routing, teacher_routing)
```

### æ€»æŸå¤±
```
Total_loss = Î±*KD_loss + Î²*Hard_loss + Î³*Feature_loss + Î´*Expert_loss + Îµ*Cache_loss + Î¶*Routing_loss
```

## é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ•™å¸ˆæ¨¡å‹

```python
from distillation import create_teacher_model

# åˆ›å»ºè‡ªå®šä¹‰æ•™å¸ˆæ¨¡å‹
teacher_model = create_teacher_model(
    hidden_size=config['hidden_size'] * 2,
    num_experts=config['num_experts'],
    num_layers=6
)

# åŠ è½½é¢„è®­ç»ƒæƒé‡
teacher_model.load_state_dict(torch.load('teacher_weights.pth'))
```

### æ¸è¿›å¼è’¸é¦

```python
# é˜¶æ®µ1: é«˜æ¸©åº¦è’¸é¦
student_model.distillation_module.kd_loss.temperature = 8.0
# è®­ç»ƒå‡ ä¸ªepoch...

# é˜¶æ®µ2: ä¸­ç­‰æ¸©åº¦è’¸é¦
student_model.distillation_module.kd_loss.temperature = 4.0
# ç»§ç»­è®­ç»ƒ...

# é˜¶æ®µ3: ä½æ¸©åº¦è’¸é¦
student_model.distillation_module.kd_loss.temperature = 2.0
# æœ€ç»ˆè®­ç»ƒ...
```

### é€‰æ‹©æ€§è’¸é¦

```python
# åªå¯ç”¨ç‰¹å®šç±»å‹çš„è’¸é¦
student_model.distillation_module.expert_distill_weight = 0.0  # ç¦ç”¨ä¸“å®¶è’¸é¦
student_model.distillation_module.cache_distill_weight = 1.0   # å¼ºåŒ–ç¼“å­˜è’¸é¦
```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. è®­ç»ƒæ—¶
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ (`torch.cuda.amp`)
- é€‚å½“çš„æ‰¹æ¬¡å¤§å°å’Œå­¦ä¹ ç‡
- æ¢¯åº¦ç´¯ç§¯ä»¥å¤„ç†å¤§æ‰¹æ¬¡

### 2. æ¨ç†æ—¶
- å§‹ç»ˆè°ƒç”¨ `disable_distillation()`
- ä½¿ç”¨ `model.eval()` æ¨¡å¼
- è€ƒè™‘æ¨¡å‹é‡åŒ–

### 3. å†…å­˜ä¼˜åŒ–
- æ•™å¸ˆæ¨¡å‹ä½¿ç”¨ `torch.no_grad()`
- åŠæ—¶é‡Šæ”¾ä¸éœ€è¦çš„ä¸­é—´å˜é‡
- ä½¿ç”¨æ£€æŸ¥ç‚¹æŠ€æœ¯

## å®éªŒç»“æœ

åŸºäºæˆ‘ä»¬çš„æµ‹è¯•ï¼ŒçŸ¥è¯†è’¸é¦åœ¨PiKV MoEä¸Šçš„æ•ˆæœï¼š

| æ¨¡å‹ç±»å‹ | æµ‹è¯•æŸå¤± | å‡†ç¡®ç‡ | æ¨ç†é€Ÿåº¦ |
|---------|---------|--------|----------|
| æ ‡å‡†PiKV | 2.45 | 0.72 | 100% |
| PiKV + è’¸é¦ | 2.31 | 0.75 | 98% |
| æ ‡å‡†MoE | 2.58 | 0.69 | 95% |
| LoRA PiKV | 2.38 | 0.73 | 102% |

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   - å‡å°æ‰¹æ¬¡å¤§å°
   - é™ä½æ•™å¸ˆæ¨¡å‹å¤æ‚åº¦
   - ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹

2. **è®­ç»ƒä¸ç¨³å®š**
   - é™ä½å­¦ä¹ ç‡
   - è°ƒæ•´æ¸©åº¦å‚æ•°
   - æ£€æŸ¥æŸå¤±æƒé‡å¹³è¡¡

3. **æ€§èƒ½ä¸‹é™**
   - ç¡®ä¿æ¨ç†æ—¶ç¦ç”¨è’¸é¦
   - æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ­£ç¡®åŠ è½½
   - éªŒè¯æ•°æ®é¢„å¤„ç†

### è°ƒè¯•æŠ€å·§

```python
# æ‰“å°è¯¦ç»†æŸå¤±ä¿¡æ¯
loss_info = student_model.distillation_step(...)
for name, value in loss_info.items():
    print(f"{name}: {value:.6f}")

# æ£€æŸ¥æ¨¡å‹çŠ¶æ€
print(f"è’¸é¦çŠ¶æ€: {student_model.use_distillation}")
print(f"è®­ç»ƒæ¨¡å¼: {student_model.training}")

# éªŒè¯æ•™å¸ˆæ¨¡å‹
with torch.no_grad():
    teacher_output = student_model.teacher_model(test_input)
    print(f"æ•™å¸ˆè¾“å‡ºå½¢çŠ¶: {teacher_output['logits'].shape}")
```

## æ‰©å±•å’Œå®šåˆ¶

### æ·»åŠ æ–°çš„è’¸é¦ç­–ç•¥

```python
class CustomDistillationLoss(nn.Module):
    def __init__(self):
        super().__init__()
        # è‡ªå®šä¹‰åˆå§‹åŒ–
    
    def forward(self, student_output, teacher_output):
        # å®ç°è‡ªå®šä¹‰è’¸é¦æŸå¤±
        return custom_loss

# é›†æˆåˆ°PiKVDistillationä¸­
```

### å¤šæ•™å¸ˆè’¸é¦

```python
# æ‰©å±•æ”¯æŒå¤šä¸ªæ•™å¸ˆæ¨¡å‹
class MultiTeacherDistillation(PiKVDistillation):
    def __init__(self, teachers):
        super().__init__()
        self.teachers = teachers
    
    def forward(self, student_output, **kwargs):
        # å®ç°å¤šæ•™å¸ˆè’¸é¦é€»è¾‘
        pass
```

## å‚è€ƒæ–‡çŒ®

1. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. (2015)
2. Romero, A., et al.: Fitnets: Hints for thin deep nets. ICLR (2015)
3. PyTorch Knowledge Distillation Tutorial
4. DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model

## æ›´æ–°æ—¥å¿—

- **v1.0**: åˆå§‹å®ç°ï¼Œæ”¯æŒåŸºç¡€çŸ¥è¯†è’¸é¦
- **v1.1**: æ·»åŠ ä¸“å®¶çº§å’Œç¼“å­˜è’¸é¦
- **v1.2**: é›†æˆè·¯ç”±è’¸é¦å’Œå¤šå±‚æ¬¡æŸå¤±
- **v1.3**: ä¼˜åŒ–æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨

---

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æäº¤Issueæˆ–Pull Requestã€‚ 