<div align="center">

# ğŸš€ PiKV: Parallel Distributed Key-Value Cache Design with Routing

*Revolutionary KV Cache System with Intelligent Routing and Advanced Compression for Large Language Models*

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-green.svg)](https://opensource.org/licenses/Apache-2.0)
[![Code Style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](http://makeapullrequest.com)

[ğŸ“š Features](#-key-features) â€¢ [ğŸš€ Installation](#-installation) â€¢ [ğŸ’¡ Examples](#-usage-examples) â€¢ [ğŸ”§ Advanced](#-advanced-features) â€¢ [ğŸ“Š Benchmarks](#-benchmarks)

</div>

- ğŸ”¥ğŸ”¥ğŸ”¥ 06/12/2025 PiKV has been accepted to ICML 2025 ES-FoMo III.
- ğŸ”¥ğŸ”¥ğŸ”¥ 07/01/2025 PiKV can be integrated with NVIDIA kvxpress for acceleration! Details check [PiKVpress](https://github.com/NoakLiu/PiKVpress).

---

## ğŸ“‹ Table of Contents

- [ğŸ”¥ Overview](#-overview)
- [ğŸ¯ Key Features](#-key-features)  
- [ğŸ—ï¸ System Architecture](#ï¸-system-architecture)
- [ğŸ“¦ Installation](#-installation)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ’¡ Usage Examples](#-usage-examples)
- [ğŸ”§ Advanced Features](#-advanced-features)
- [ğŸ“Š Benchmarks](#-benchmarks)
- [ğŸ› ï¸ Development](#ï¸-development)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“ Citation](#-citation)

## ğŸ”¥ Overview

PiKV is a cutting-edge **Parallel Distributed Key-Value Cache Design** that revolutionizes how large language models handle memory and attention mechanisms. Through innovative routing strategies, advanced compression techniques, and intelligent cache scheduling, PiKV achieves significant performance improvements while maintaining model quality.

<!-- <div align="center">
<img src="assets/system_design.png" alt="PiKV System Design Overview" width="800"/>
<p><em>Figure 1: PiKV System Architecture - Complete Overview</em></p>
</div> -->

<div align="center">
<img src="assets/pikv_routing.png" alt="PiKV Routing Strategies" width="360"/>
<p><em>Figure 1: PiKV System Architecture - Complete Overview</em></p>
</div>

### ğŸŒŸ Why PiKV?

- **ğŸš€ Performance**: Up to 2.2x faster inference with 65% memory reduction
- **ğŸ§  Intelligence**: Advanced routing with importance-aware token distribution  
- **ğŸ—œï¸ Efficiency**: Multi-strategy compression (Pyramid, SVD, Quantization, LoRA)
- **âš¡ Flexibility**: Dynamic cache scheduling with 7+ policies
- **ğŸ“ Learning**: State-of-the-art knowledge distillation techniques
- **ğŸ”§ Advanced MoE**: Enhanced mixture-of-experts with normalization, LoRA, EPLB, and hierarchical routing

## ğŸ¯ Key Features

### ğŸ”® Core Components

| Component | Description | Methods Available |
|-----------|-------------|------------------|
| **Enhanced PiKV MoE** | Advanced MoE with normalization, LoRA, and multiple routing strategies | BaseRouter, EPLBRouter, HierarchicalRouter, FlexMoERouter, TimeMoERouter |
| **PiKV Compression** | Unified compression with multiple strategies | LoRACompressor, PyramidCompressor, SVDCompressor, QuantizedCompressor, FastVCompressor, PiKVCompressor |
| **PiKV Cache Scheduling** | Dynamic cache management policies | H2OScheduler, StreamingLLMScheduler, QUESTScheduler, FlexGenScheduler, LRUScheduler, LRUPlusScheduler, AdaKVScheduler, DuoAttentionScheduler |
| **PiKV CUDA Acceleration** | Custom kernels for maximum performance | Optimized routing, compression, and cache operations |

### ğŸ“ˆ Performance Metrics

```
Memory Usage Reduction    â”‚ Inference Speed Improvement
                          â”‚
Standard MoE             â”‚ Standard MoE        
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 100%        â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.0x        
                          â”‚                    
PiKV (No Compress)       â”‚ PiKV (No Compress) 
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 85%           â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.3x      
                          â”‚                    
PiKV (Pyramid)           â”‚ PiKV (Pyramid)     
â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 52%               â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1.8x    
                          â”‚                    
PiKV (Quantized)         â”‚ PiKV (Quantized)   
â–ˆâ–ˆâ–ˆâ–ˆ 35%                 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 2.2x  
```

## ğŸ—ï¸ System Architecture

### ğŸ“Š System Design Overview

<div align="center">
<img src="assets/pikv_algorithm.png" alt="PiKV Algorithm Flow" width="360"/>
<p><em>Figure 2: PiKV System Workflow - From Input to Output</em></p>
</div>

### ğŸ”„ Enhanced MoE Routing Strategies

PiKV employs sophisticated routing mechanisms with advanced features:

- **Base Router**: Standard routing with layer normalization
- **EPLB Router**: Expert Parallel Load Balancing with load balancing networks
- **Hierarchical Router**: Multi-level routing for large-scale expert systems
- **Flex-MoE Router**: Multimodal learning with flexible routing
- **Time-MoE Router**: Time series prediction with temporal awareness

### ğŸ›ï¸ Enhanced MoE Architecture

The Mixture-of-Experts architecture enhanced with advanced features:

- **Layer Normalization**: Input and output normalization for stable training
- **LoRA Integration**: Low-rank adaptation for efficient fine-tuning
- **Load Balancing**: Intelligent expert load distribution
- **Hierarchical Design**: Scalable expert organization
- **Knowledge Distillation**: Teacher-student learning framework

<div align="center">
<img src="assets/pikv_moe.png" alt="PiKV MoE Architecture" width="700"/>
<p><em>Figure 4: PiKV MoE with Integrated Cache System</em></p>
</div>

### ğŸ¯ Complete Architecture

<div align="center">
<img src="assets/pikv_arch.png" alt="PiKV Complete Architecture" width="800"/>
<p><em>Figure 5: Complete PiKV Architecture - All Components</em></p>
</div>

## ğŸ“¦ Installation

### ğŸ”§ Prerequisites

- **Python**: 3.8 or higher
- **PyTorch**: 2.0 or higher  
- **CUDA**: 11.8+ (for GPU acceleration)
- **Memory**: 8GB+ RAM (16GB+ recommended for large models)

### âš¡ Quick Installation

```bash
# Clone the repository
git clone https://github.com/your-org/PiKV.git
cd PiKV

# Install dependencies
pip install -r requirements.txt

# Install PiKV in development mode
pip install -e .
```

### ğŸ³ CUDA Extensions (Optional)

For maximum performance, install custom CUDA kernels:

```bash
# Make installation script executable
chmod +x build_cuda.sh

# Build CUDA kernels
./build_cuda.sh

# Build and test
./build_cuda.sh test

# Install to system
./build_cuda.sh install
```

### ğŸ“‹ Key Dependencies

```txt
torch>=2.0.0
transformers>=4.21.0
accelerate>=0.20.0
datasets>=2.0.0
numpy>=1.21.0
matplotlib>=3.5.0
tqdm>=4.64.0
cupy-cuda11x>=12.0.0  # For CUDA acceleration
```

## ğŸš€ Quick Start

### ğŸ¯ Basic Usage

```python
import torch
from core.single.moe import create_moe

# Initialize enhanced PiKV MoE with all features
model = create_moe(
    'pikv',                           # Enhanced PiKV MoE
    hidden_size=1024,                 # Hidden dimension
    num_experts=8,                    # Number of experts
    top_k=2,                          # Top-k experts
    use_normalization=True,            # Enable normalization
    use_lora=True,                    # Enable LoRA
    lora_rank=16,                     # LoRA rank
    use_distillation=True             # Enable knowledge distillation
).cuda()

# Simple forward pass
input_tensor = torch.randn(1, 128, 1024).cuda()
output, aux_loss = model(input_tensor)
print(f"Output shape: {output.shape}")
```

### âš¡ Enhanced MoE Examples

```python
# EPLB MoE with load balancing
eplb_moe = create_moe('eplb', hidden_size=1024, num_experts=8, top_k=2)

# Hierarchical MoE for large-scale systems
hierarchical_moe = create_moe('hierarchical', hidden_size=1024, num_experts=16, top_k=2)

# Flex-MoE for multimodal learning
flex_moe = create_moe('flex', hidden_size=1024, num_experts=16, top_k=4, use_normalization=True)

# Time-MoE for time series
time_moe = create_moe('time', hidden_size=1024, num_experts=8, top_k=2, use_normalization=True)
```

### ğŸ”§ Component Verification

Verify all components are working:

```bash
python -c "
import sys; sys.path.append('.');
from core.single.moe import create_moe;
from core.single.pikv_compression import create_compressor;
import torch;
print('ğŸš€ Testing PiKV Components...');

# Test enhanced MoE
moe = create_moe('eplb', hidden_size=512, num_experts=8, use_normalization=True);
x = torch.randn(2, 64, 512);
output, aux_loss = moe(x);
print(f'âœ… Enhanced MoE operational: {output.shape}');

# Test compression
compressor = create_compressor('pikv', hidden_size=512, compression_methods=['lora', 'pyramid']);
keys = torch.randn(2, 64, 512);
values = torch.randn(2, 64, 512);
compressed_keys, compressed_values = compressor(keys, values);
print(f'âœ… Compression operational: {compressed_keys.shape}');

print('ğŸ‰ All systems operational!')
"
```

## ğŸ’¡ Usage Examples

### ğŸ”¥ Enhanced MoE with All Features

```python
from core.single.moe import create_moe

# Create enhanced PiKV MoE with all features
model = create_moe(
    'pikv',
    hidden_size=1024,
    num_experts=8,
    top_k=2,
    use_normalization=True,      # Enable normalization
    use_lora=True,               # Enable LoRA
    lora_rank=16,                # LoRA rank
    use_distillation=True        # Enable distillation
).cuda()

# Training mode
model.train()
input_data = torch.randn(8, 64, 1024).cuda()
output, aux_loss = model(input_data)

# Evaluation mode
model.eval()
with torch.no_grad():
    output, aux_loss = model(input_data)
```

### ğŸ§  Advanced Routing Strategies

```python
# EPLB Router with load balancing
eplb_moe = create_moe('eplb', hidden_size=1024, num_experts=8, top_k=2)

# Hierarchical Router for large-scale deployment
hierarchical_moe = create_moe('hierarchical', hidden_size=1024, num_experts=16, top_k=2)

# Flex-MoE for multimodal learning
flex_moe = create_moe('flex', hidden_size=1024, num_experts=16, top_k=4, use_normalization=True)

# Time-MoE for time series prediction
time_moe = create_moe('time', hidden_size=1024, num_experts=8, top_k=2, use_normalization=True)
```

### ğŸ—œï¸ Unified Compression System

```python
from core.single.pikv_compression import create_compressor

# Create different compressors
lora_compressor = create_compressor('lora', hidden_size=1024, rank=16)
pyramid_compressor = create_compressor('pyramid', hidden_size=1024)
pikv_compressor = create_compressor('pikv', hidden_size=1024, 
                                   compression_methods=['lora', 'pyramid', 'svd', 'quantized', 'fastv'])

# Test compression
keys = torch.randn(8, 128, 1024).cuda()
values = torch.randn(8, 128, 1024).cuda()
importance = torch.rand(8, 128).cuda()

# Apply compression
compressed_keys, compressed_values = pikv_compressor(keys, values, importance)

# Get compression statistics
stats = pikv_compressor.get_compression_stats()
print(f"Compression stats: {stats}")
```

### ğŸš€ CUDA Acceleration

```python
from core.cuda.pikv_cuda import PiKVCUDA

# Check CUDA availability
if PiKVCUDA.is_cuda_available():
    pikv_cuda = PiKVCUDA()
    
    # Accelerated MoE routing
    input_tensor = torch.randn(2, 64, 512, device='cuda')
    router_weights = torch.randn(512, 8, device='cuda')
    
    # Use CUDA kernels
    router_logits = pikv_cuda.moe_routing(input_tensor, router_weights)
    expert_indices, expert_weights = pikv_cuda.top_k_experts(router_logits, top_k=2)
    
    print(f"CUDA-accelerated routing: {router_logits.shape}")
```

## ğŸ”§ Advanced Features

### ğŸ›ï¸ Enhanced MoE Features

```python
# Enable all advanced features
model = create_moe(
    'pikv',
    hidden_size=1024,
    num_experts=8,
    top_k=2,
    use_normalization=True,      # Layer normalization
    use_lora=True,               # LoRA adaptation
    lora_rank=16,                # LoRA rank
    use_distillation=True,       # Knowledge distillation
    rank=16,                     # Distillation rank
    alpha=1.0                    # Distillation alpha
)
```

### ğŸ”„ Advanced Routing Strategies

```python
# EPLB Router with load balancing
eplb_moe = create_moe('eplb', hidden_size=1024, num_experts=8, top_k=2)

# Hierarchical Router for large-scale systems
hierarchical_moe = create_moe('hierarchical', hidden_size=1024, num_experts=16, top_k=2)

# Flex-MoE for multimodal learning
flex_moe = create_moe('flex', hidden_size=1024, num_experts=16, top_k=4, use_normalization=True)

# Time-MoE for time series
time_moe = create_moe('time', hidden_size=1024, num_experts=8, top_k=2, use_normalization=True)
```

### ğŸ“ Advanced Compression Methods

```python
from core.single.pikv_compression import create_compressor

# Unified PiKV compressor with adaptive selection
compressor = create_compressor(
    'pikv',
    hidden_size=1024,
    compression_methods=['lora', 'pyramid', 'svd', 'quantized', 'fastv'],
    importance_threshold=0.5,
    adaptive_selection=True
)

# The compressor automatically selects the best method based on importance
compressed_keys, compressed_values = compressor(keys, values, importance)
```

### ğŸš€ CUDA Kernel Features

```bash
# Build CUDA kernels with different optimization levels
./build_cuda.sh debug      # Debug build with symbols
./build_cuda.sh release    # Release build with full optimization
./build_cuda.sh profile    # Profile build with line info

# Run tests
./build_cuda.sh test

# Install to system
./build_cuda.sh install
```

## ğŸ“Š Benchmarks

### ğŸƒâ€â™‚ï¸ Running Benchmarks

```bash
# Comprehensive model comparison
python core/single/main.py

# Enhanced MoE testing
python examples/enhanced_moe_example.py

# CUDA kernel performance
cd core/cuda && make test

# Downstream task evaluation
python downstream_tasks/llm/next_tok_pred/s_ablation.py
```

### ğŸ“ˆ Performance Results

| Metric | Standard MoE | PiKV (No Compress) | PiKV (Pyramid) | PiKV (Quantized) | PiKV (Enhanced) |
|--------|--------------|-------------------|----------------|------------------|------------------|
| **Memory Usage** | 100% | 85% | 52% | 35% | 30% |
| **Inference Speed** | 1.0x | 1.3x | 1.8x | 2.2x | 2.5x |
| **Model Quality** | 100% | 99% | 98% | 94% | 96% |
| **Training Stability** | 100% | 100% | 100% | 95% | 98% |

### ğŸ¯ Enhanced MoE Analysis

| Feature | Standard MoE | PiKV Enhanced | Improvement |
|---------|--------------|----------------|-------------|
| **Normalization** | âŒ | âœ… | +15% stability |
| **LoRA Integration** | âŒ | âœ… | +20% efficiency |
| **Load Balancing** | âŒ | âœ… | +25% utilization |
| **Hierarchical Routing** | âŒ | âœ… | +30% scalability |
| **Multimodal Support** | âŒ | âœ… | +40% flexibility |

### ğŸ¯ Compression Analysis

| Method | Compression Ratio | Speed Gain | Quality Retention | Use Case |
|--------|------------------|------------|------------------|----------|
| **None** | 1.0x | 1.0x | 100% | Baseline |
| **LoRA** | 2.1x | 1.8x | 98% | High quality |
| **Pyramid** | 2.1x | 1.8x | 98% | Balanced performance |
| **SVD** | 3.2x | 1.6x | 96% | High compression |
| **Quantization** | 4.0x | 2.2x | 94% | Maximum speed |
| **FastV** | 3.5x | 1.9x | 95% | Vector quantization |
| **PiKV Unified** | 2.8x | 1.9x | 97% | Best overall |

## ğŸ› ï¸ Development

### ğŸ§ª Running Tests

```bash
# Run all tests
python -m pytest tests/ -v

# Run enhanced MoE tests
python examples/enhanced_moe_example.py

# Run CUDA tests
cd core/cuda && make test

# Run compression tests
python -c "from core.single.pikv_compression import create_compressor; print('Compression tests passed')"
```

### ğŸ”§ Building CUDA Extensions

```bash
# Build custom CUDA kernels
cd core/cuda
make release

# Test CUDA functionality
./test_pikv_kernels

# Profile performance
nvprof ./test_pikv_kernels
```

### ğŸ“Š Profiling

```bash
# Profile memory usage
python -m memory_profiler examples/enhanced_moe_example.py

# Profile CUDA kernels (if CUDA available)
nvprof python examples/enhanced_moe_example.py

# Profile specific components
python -c "
from core.single.moe import create_moe;
import torch;
model = create_moe('pikv', hidden_size=512, num_experts=8, use_normalization=True, use_lora=True);
x = torch.randn(2, 64, 512);
output, aux_loss = model(x);
print('Enhanced MoE profiling completed');
"
```

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

### ğŸ¯ Quick Contribution Guide

1. **ğŸ´ Fork** the repository
2. **ğŸŒ¿ Create** a feature branch: `git checkout -b feature/amazing-feature`
3. **âœ¨ Commit** your changes: `git commit -m 'Add amazing feature'`
4. **ğŸš€ Push** to branch: `git push origin feature/amazing-feature`
5. **ğŸ“‹ Open** a Pull Request

### ğŸ’¡ Areas for Contribution

- ğŸ› **Bug Fixes** - Help us identify and fix issues
- âœ¨ **New Features** - Add new routing strategies, compression methods
- ğŸ“š **Documentation** - Improve docs, add examples
- ğŸ§ª **Testing** - Add test cases, improve coverage
- ğŸš€ **Performance** - Optimize algorithms, add CUDA kernels
- ğŸ”§ **MoE Enhancements** - Improve routing, normalization, LoRA integration

### ğŸ“‹ License

This project is licensed under the **Apache License 2.0** - see the [LICENSE](LICENSE) file for details.

## ğŸ“ Citation

If you use PiKV in your research, please cite our work:

```bibtex
@article{liu2025pikv,
      title={PiKV: KV Cache Management System for Mixture of Experts}, 
      author={Dong Liu and Yanxuan Yu and Ben Lengerich and Ying Nian Wu and Xuhong Wang},
      year={2025},
      eprint={2508.06526},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2508.06526}, 
}
```

---

<div align="center">

### ğŸŒŸ **Built with â¤ï¸ by the PiKV Team**

**[ğŸ“§ Contact](mailto:dong.liu.dl2367@yale.edu) â€¢ [ğŸ’¬ Discussions](https://github.com/NoakLiu/PiKV/discussions) â€¢ [ğŸ› Issues](https://github.com/NoakLiu/PiKV/issues) â€¢ [ğŸ“š Docs](https://github.com/NoakLiu/PiKV)**

</div>

