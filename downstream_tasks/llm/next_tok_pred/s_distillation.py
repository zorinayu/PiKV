#!/usr/bin/env python3
"""
æµ‹è¯•PiKVåˆ†å¸ƒå¼çŸ¥è¯†è’¸é¦åŠŸèƒ½
"""

import torch
import os
import sys
import subprocess
import tempfile
from pathlib import Path

def test_single_gpu_distillation():
    """æµ‹è¯•å•GPUè’¸é¦åŠŸèƒ½"""
    print("=" * 60)
    print("æµ‹è¯•å•GPUçŸ¥è¯†è’¸é¦åŠŸèƒ½")
    print("=" * 60)
    
    try:
        # è®¾ç½®ç¯å¢ƒå˜é‡
        os.environ['RANK'] = '0'
        os.environ['WORLD_SIZE'] = '1'
        os.environ['LOCAL_RANK'] = '0'
        os.environ['MASTER_ADDR'] = 'localhost'
        os.environ['MASTER_PORT'] = '12345'
        
        # å¯¼å…¥æ¨¡å—
        from d_transformers_distillation import DistributedPiKVCacheWithDistillation
        
        # åˆå§‹åŒ–æ¨¡å‹
        print("åˆå§‹åŒ–DistributedPiKVCacheWithDistillation...")
        pikv_cache = DistributedPiKVCacheWithDistillation(
            model_name="gpt2",
            max_length=512,
            use_distillation=True,
            teacher_hidden_size=1536,
            distillation_temperature=4.0,
            distillation_alpha=0.7
        )
        
        print("âœ“ æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç”ŸæˆåŠŸèƒ½
        print("\næµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        test_prompt = "The future of artificial intelligence"
        generated_text = pikv_cache.generate_with_distillation(
            test_prompt,
            max_new_tokens=20,
            temperature=0.7,
            use_teacher=True
        )
        
        print(f"è¾“å…¥: {test_prompt}")
        print(f"è¾“å‡º: {generated_text}")
        print("âœ“ æ–‡æœ¬ç”ŸæˆæˆåŠŸ")
        
        # æµ‹è¯•è®­ç»ƒæ­¥éª¤
        print("\næµ‹è¯•è’¸é¦è®­ç»ƒæ­¥éª¤...")
        
        # åˆ›å»ºè™šæ‹Ÿè®­ç»ƒæ•°æ®
        batch_size = 2
        seq_len = 10
        vocab_size = pikv_cache.tokenizer.vocab_size
        
        input_data = torch.randint(0, vocab_size, (batch_size, seq_len), device=pikv_cache.device)
        targets = torch.randint(0, vocab_size, (batch_size, seq_len), device=pikv_cache.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        optimizer = torch.optim.Adam(pikv_cache.model.parameters(), lr=1e-4)
        
        # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
        loss_info = pikv_cache.distillation_training_step(
            input_data=input_data,
            targets=targets,
            optimizer=optimizer
        )
        
        print("è®­ç»ƒæŸå¤±ä¿¡æ¯:")
        for loss_name, loss_value in loss_info.items():
            print(f"  {loss_name}: {loss_value:.4f}")
        
        print("âœ“ è’¸é¦è®­ç»ƒæ­¥éª¤æˆåŠŸ")
        
        # æµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½
        print("\næµ‹è¯•æ£€æŸ¥ç‚¹ä¿å­˜å’ŒåŠ è½½...")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            checkpoint_path = os.path.join(temp_dir, "test_checkpoint.pth")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            pikv_cache.save_checkpoint(checkpoint_path)
            print(f"âœ“ æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {checkpoint_path}")
            
            # åŠ è½½æ£€æŸ¥ç‚¹
            pikv_cache.load_checkpoint(checkpoint_path)
            print("âœ“ æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
        
        print("\n" + "=" * 60)
        print("å•GPUè’¸é¦æµ‹è¯•å…¨éƒ¨é€šè¿‡ï¼")
        print("=" * 60)
        
        return True
        
    except Exception as e:
        print(f"âŒ å•GPUè’¸é¦æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_torchrun_command():
    """æµ‹è¯•torchrunå‘½ä»¤æ˜¯å¦å¯ä»¥æ­£å¸¸æ‰§è¡Œ"""
    print("=" * 60)
    print("æµ‹è¯•torchrunå‘½ä»¤æ‰§è¡Œ")
    print("=" * 60)
    
    try:
        # æ„å»ºtorchrunå‘½ä»¤
        script_path = Path(__file__).parent / "d_transformers_distillation.py"
        
        cmd = [
            "torchrun",
            "--nproc_per_node=1",
            "--nnodes=1", 
            "--node_rank=0",
            "--master_addr=localhost",
            "--master_port=23456",
            str(script_path),
            "--use_distillation",
            "--model", "gpt2",
            "--max_tokens", "10"
        ]
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        
        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=120  # 2åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            print("âœ“ torchrunå‘½ä»¤æ‰§è¡ŒæˆåŠŸ")
            print("æ ‡å‡†è¾“å‡º:")
            print(result.stdout[-500:])  # æ˜¾ç¤ºæœ€å500ä¸ªå­—ç¬¦
            return True
        else:
            print(f"âŒ torchrunå‘½ä»¤æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            print("æ ‡å‡†é”™è¯¯:")
            print(result.stderr)
            return False
            
    except subprocess.TimeoutExpired:
        print("âŒ torchrunå‘½ä»¤æ‰§è¡Œè¶…æ—¶")
        return False
    except Exception as e:
        print(f"âŒ torchrunå‘½ä»¤æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_multi_gpu_simulation():
    """æ¨¡æ‹Ÿå¤šGPUç¯å¢ƒæµ‹è¯•"""
    print("=" * 60)
    print("æ¨¡æ‹Ÿå¤šGPUç¯å¢ƒæµ‹è¯•")
    print("=" * 60)
    
    try:
        # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
        if not torch.cuda.is_available():
            print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å¤šGPUæµ‹è¯•")
            return True
        
        gpu_count = torch.cuda.device_count()
        print(f"æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
        
        if gpu_count < 2:
            print("âš ï¸  GPUæ•°é‡ä¸è¶³ï¼Œè·³è¿‡å¤šGPUæµ‹è¯•")
            return True
        
        # æ„å»ºå¤šGPU torchrunå‘½ä»¤
        script_path = Path(__file__).parent / "d_transformers_distillation.py"
        
        cmd = [
            "torchrun",
            f"--nproc_per_node={min(2, gpu_count)}",
            "--nnodes=1",
            "--node_rank=0", 
            "--master_addr=localhost",
            "--master_port=23457",
            str(script_path),
            "--use_distillation",
            "--model", "gpt2",
            "--max_tokens", "5"
        ]
        
        print(f"æ‰§è¡Œå¤šGPUå‘½ä»¤: {' '.join(cmd)}")
        
        # æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=180  # 3åˆ†é’Ÿè¶…æ—¶
        )
        
        if result.returncode == 0:
            print("âœ“ å¤šGPUæµ‹è¯•æˆåŠŸ")
            return True
        else:
            print(f"âŒ å¤šGPUæµ‹è¯•å¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            print("æ ‡å‡†é”™è¯¯:")
            print(result.stderr[-1000:])  # æ˜¾ç¤ºæœ€å1000ä¸ªå­—ç¬¦
            return False
            
    except Exception as e:
        print(f"âŒ å¤šGPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–é¡¹"""
    print("=" * 60)
    print("æ£€æŸ¥ä¾èµ–é¡¹")
    print("=" * 60)
    
    dependencies = [
        "torch",
        "transformers", 
        "numpy"
    ]
    
    missing_deps = []
    
    for dep in dependencies:
        try:
            __import__(dep)
            print(f"âœ“ {dep}")
        except ImportError:
            print(f"âŒ {dep} (ç¼ºå¤±)")
            missing_deps.append(dep)
    
    # æ£€æŸ¥torchrun
    try:
        result = subprocess.run(["torchrun", "--help"], capture_output=True)
        if result.returncode == 0:
            print("âœ“ torchrun")
        else:
            print("âŒ torchrun (ä¸å¯ç”¨)")
            missing_deps.append("torchrun")
    except FileNotFoundError:
        print("âŒ torchrun (æœªæ‰¾åˆ°)")
        missing_deps.append("torchrun")
    
    if missing_deps:
        print(f"\nç¼ºå¤±ä¾èµ–é¡¹: {', '.join(missing_deps)}")
        print("è¯·å®‰è£…ç¼ºå¤±çš„ä¾èµ–é¡¹åé‡æ–°è¿è¡Œæµ‹è¯•")
        return False
    
    print("\nâœ“ æ‰€æœ‰ä¾èµ–é¡¹æ£€æŸ¥é€šè¿‡")
    return True

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("PiKV åˆ†å¸ƒå¼çŸ¥è¯†è’¸é¦æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    # æ£€æŸ¥ä¾èµ–é¡¹
    if not check_dependencies():
        sys.exit(1)
    
    test_results = []
    
    # è¿è¡Œæµ‹è¯•
    tests = [
        ("å•GPUè’¸é¦åŠŸèƒ½", test_single_gpu_distillation),
        ("torchrunå‘½ä»¤æ‰§è¡Œ", test_torchrun_command),
        ("å¤šGPUç¯å¢ƒæ¨¡æ‹Ÿ", test_multi_gpu_simulation)
    ]
    
    for test_name, test_func in tests:
        print(f"\nå¼€å§‹æµ‹è¯•: {test_name}")
        try:
            result = test_func()
            test_results.append((test_name, result))
        except Exception as e:
            print(f"æµ‹è¯• {test_name} å‡ºç°å¼‚å¸¸: {e}")
            test_results.append((test_name, False))
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("æµ‹è¯•ç»“æœæ±‡æ€»")
    print("=" * 60)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ“ é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼åˆ†å¸ƒå¼è’¸é¦åŠŸèƒ½æ­£å¸¸å·¥ä½œã€‚")
        sys.exit(0)
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯ã€‚")
        sys.exit(1)

if __name__ == "__main__":
    main() 